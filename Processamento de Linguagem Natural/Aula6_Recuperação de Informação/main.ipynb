{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Recuperação de Informação\n",
    "\n",
    "## Título da Prática: Recuperação de Informação em Textos\n",
    "\n",
    "### Objetivos\n",
    "\n",
    "- Implementar técnicas de recuperação de informação.\n",
    "- Utilizar a distância de Levenshtein para recuperação de informação.\n",
    "- Utilizar o modelo bag-of-words para recuperação de informação.\n",
    "\n",
    "### Materiais, Métodos e Ferramentas\n",
    "\n",
    "- Ambiente de desenvolvimento com Python.\n",
    "- Bibliotecas: NLTK, sklearn, numpy para processamento de linguagem natural e cálculos.\n",
    "- Conjuntos de Dados: Textos diversos para análise prática.\n",
    "\n",
    "## Atividade Prática\n",
    "\n",
    "Nesta atividade, você implementará técnicas de recuperação de informação utilizando distância de Levenshtein e modelo bag-of-words. De forma geral, são 3 grandes etapas no desenvolvimento do código, são elas:\n",
    "\n",
    "1. **Segmentação do Texto e Tokenização:**\n",
    "   - Faça download usando o NLTK dos pacotes: `punkt` e `stopwords`.\n",
    "   - Implemente os textos de exemplo, sendo um o texto e o outro a query.\n",
    "\n",
    "   **Texto:** \n",
    "   “A crescente quantidade de dados textuais gerada diariamente tem impulsionado a importância do processamento de linguagem natural (PLN) e das técnicas de recuperação de informação (RI). Com o objetivo de extrair informações relevantes de grandes volumes de dados, a RI utiliza métodos avançados para encontrar e classificar documentos que atendam a consultas específicas dos usuários. A distância de Levenshtein e o modelo bag-of-words são duas abordagens distintas, mas eficazes, para medir a similaridade entre textos e consultas. A distância de Levenshtein calcula o número mínimo de operações necessárias para transformar uma string em outra, enquanto o modelo bag-of-words representa os textos como vetores de frequência de palavras, permitindo a comparação baseada na similaridade vetorial. Essas técnicas são fundamentais para melhorar a precisão dos sistemas de busca e a relevância dos resultados apresentados aos usuários.”\n",
    "\n",
    "   **Query:** \n",
    "   “importância do processamento de linguagem natural”.\n",
    "\n",
    "2. **Implementação de Recuperação de Informação Utilizando Distância de Levenshtein:**\n",
    "   - Implemente a função `ri_levenshtein` para calcular a distância de Levenshtein entre a consulta e os documentos.\n",
    "   - Imprima no terminal o resultado com as distâncias.\n",
    "\n",
    "3. **Implementação de Recuperação de Informação Utilizando Bag-of-Words:**\n",
    "   - Implemente a função `ri_bag_of_words` para calcular a similaridade entre a consulta e os documentos usando o modelo bag-of-words.\n",
    "   - Imprima no terminal as similaridades calculadas.\n",
    "\n",
    "Após finalizar a atividade, disponibilize o resultado para avaliação (via plataforma ou documento .doc ou .pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import edit_distance\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cacoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cacoc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"\n",
    "A crescente quantidade de dados textuais gerada diariamente tem impulsionado a importância do processamento de linguagem natural (PLN) e das técnicas de recuperação de informação (RI). \n",
    "Com o objetivo de extrair informações relevantes de grandes volumes de dados, a RI utiliza métodos avançados para encontrar e classificar documentos que atendam a consultas específicas dos usuários. \n",
    "A distância de Levenshtein e o modelo bag-of-words são duas abordagens distintas, mas eficazes, para medir a similaridade entre textos e consultas. \n",
    "A distância de Levenshtein calcula o número mínimo de operações necessárias para transformar uma string em outra, enquanto o modelo bag-of-words representa os textos como vetores de frequência de palavras, permitindo a comparação baseada na similaridade vetorial. \n",
    "Essas técnicas são fundamentais para melhorar a precisão dos sistemas de busca e a relevância dos resultados apresentados aos usuários.\n",
    "\"\"\"\n",
    "\n",
    "query = \"importância do processamento de linguagem natural\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Segmentação do Texto e Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens do Texto: ['crescente', 'quantidade', 'dados', 'textuais', 'gerada', 'diariamente', 'impulsionado', 'importância', 'processamento', 'linguagem', 'natural', 'pln', 'técnicas', 'recuperação', 'informação', 'ri', 'objetivo', 'extrair', 'informações', 'relevantes', 'grandes', 'volumes', 'dados', 'ri', 'utiliza', 'métodos', 'avançados', 'encontrar', 'classificar', 'documentos', 'atendam', 'consultas', 'específicas', 'usuários', 'distância', 'levenshtein', 'modelo', 'duas', 'abordagens', 'distintas', 'eficazes', 'medir', 'similaridade', 'textos', 'consultas', 'distância', 'levenshtein', 'calcula', 'número', 'mínimo', 'operações', 'necessárias', 'transformar', 'string', 'outra', 'enquanto', 'modelo', 'representa', 'textos', 'vetores', 'frequência', 'palavras', 'permitindo', 'comparação', 'baseada', 'similaridade', 'vetorial', 'técnicas', 'fundamentais', 'melhorar', 'precisão', 'sistemas', 'busca', 'relevância', 'resultados', 'apresentados', 'usuários']\n",
      "Tokens da Query: ['importância', 'processamento', 'linguagem', 'natural']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('portuguese'))\n",
    "tokens_texto = word_tokenize(texto.lower())\n",
    "tokens_query = word_tokenize(query.lower())\n",
    "\n",
    "tokens_texto_sem_stopwords = [token for token in tokens_texto if token.isalpha() and token not in stop_words]\n",
    "tokens_query_sem_stopwords = [token for token in tokens_query if token.isalpha() and token not in stop_words]\n",
    "\n",
    "print(\"Tokens do Texto:\", tokens_texto_sem_stopwords)\n",
    "print(\"Tokens da Query:\", tokens_query_sem_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementação de Recuperação de Informação Utilizando Distância de Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ri_levenshtein(query, documento):\n",
    "    return edit_distance(query, documento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "distancias = {}\n",
    "for token in tokens_texto_sem_stopwords:\n",
    "    distancias[token] = ri_levenshtein(query, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distâncias de Levenshtein:\n",
      "Token: crescente, Distância: 41\n",
      "Token: quantidade, Distância: 43\n",
      "Token: dados, Distância: 45\n",
      "Token: textuais, Distância: 44\n",
      "Token: gerada, Distância: 45\n",
      "Token: diariamente, Distância: 40\n",
      "Token: impulsionado, Distância: 42\n",
      "Token: importância, Distância: 38\n",
      "Token: processamento, Distância: 36\n",
      "Token: linguagem, Distância: 40\n",
      "Token: natural, Distância: 42\n",
      "Token: pln, Distância: 46\n",
      "Token: técnicas, Distância: 44\n",
      "Token: recuperação, Distância: 43\n",
      "Token: informação, Distância: 43\n",
      "Token: ri, Distância: 47\n",
      "Token: objetivo, Distância: 45\n",
      "Token: extrair, Distância: 44\n",
      "Token: informações, Distância: 43\n",
      "Token: relevantes, Distância: 43\n",
      "Token: grandes, Distância: 44\n",
      "Token: volumes, Distância: 45\n",
      "Token: utiliza, Distância: 44\n",
      "Token: métodos, Distância: 44\n",
      "Token: avançados, Distância: 44\n",
      "Token: encontrar, Distância: 42\n",
      "Token: classificar, Distância: 42\n",
      "Token: documentos, Distância: 41\n",
      "Token: atendam, Distância: 43\n",
      "Token: consultas, Distância: 43\n",
      "Token: específicas, Distância: 44\n",
      "Token: usuários, Distância: 45\n",
      "Token: distância, Distância: 43\n",
      "Token: levenshtein, Distância: 43\n",
      "Token: modelo, Distância: 44\n",
      "Token: duas, Distância: 46\n",
      "Token: abordagens, Distância: 41\n",
      "Token: distintas, Distância: 42\n",
      "Token: eficazes, Distância: 45\n",
      "Token: medir, Distância: 44\n",
      "Token: similaridade, Distância: 43\n",
      "Token: textos, Distância: 45\n",
      "Token: calcula, Distância: 44\n",
      "Token: número, Distância: 45\n",
      "Token: mínimo, Distância: 44\n",
      "Token: operações, Distância: 44\n",
      "Token: necessárias, Distância: 42\n",
      "Token: transformar, Distância: 42\n",
      "Token: string, Distância: 44\n",
      "Token: outra, Distância: 44\n",
      "Token: enquanto, Distância: 43\n",
      "Token: representa, Distância: 40\n",
      "Token: vetores, Distância: 44\n",
      "Token: frequência, Distância: 44\n",
      "Token: palavras, Distância: 43\n",
      "Token: permitindo, Distância: 43\n",
      "Token: comparação, Distância: 44\n",
      "Token: baseada, Distância: 44\n",
      "Token: vetorial, Distância: 43\n",
      "Token: fundamentais, Distância: 41\n",
      "Token: melhorar, Distância: 44\n",
      "Token: precisão, Distância: 43\n",
      "Token: sistemas, Distância: 43\n",
      "Token: busca, Distância: 47\n",
      "Token: relevância, Distância: 43\n",
      "Token: resultados, Distância: 44\n",
      "Token: apresentados, Distância: 40\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDistâncias de Levenshtein:\")\n",
    "for token, distancia in distancias.items():\n",
    "    print(f\"Token: {token}, Distância: {distancia}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementação de Recuperação de Informação Utilizando Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ri_bag_of_words(query_tokens, documento_tokens):\n",
    "    # Contar as palavras na query e no documento\n",
    "    counter_query = Counter(query_tokens)\n",
    "    counter_documento = Counter(documento_tokens)\n",
    "\n",
    "    # Calcular a similaridade como o número de palavras em comum\n",
    "    similaridade = sum((counter_query & counter_documento).values())\n",
    "    return similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "similaridades = {}\n",
    "for token in tokens_texto_sem_stopwords:\n",
    "    similaridades[token] = ri_bag_of_words(tokens_query_sem_stopwords, [token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similaridades Bag-of-Words:\n",
      "Token: crescente, Similaridade: 0\n",
      "Token: quantidade, Similaridade: 0\n",
      "Token: dados, Similaridade: 0\n",
      "Token: textuais, Similaridade: 0\n",
      "Token: gerada, Similaridade: 0\n",
      "Token: diariamente, Similaridade: 0\n",
      "Token: impulsionado, Similaridade: 0\n",
      "Token: importância, Similaridade: 1\n",
      "Token: processamento, Similaridade: 1\n",
      "Token: linguagem, Similaridade: 1\n",
      "Token: natural, Similaridade: 1\n",
      "Token: pln, Similaridade: 0\n",
      "Token: técnicas, Similaridade: 0\n",
      "Token: recuperação, Similaridade: 0\n",
      "Token: informação, Similaridade: 0\n",
      "Token: ri, Similaridade: 0\n",
      "Token: objetivo, Similaridade: 0\n",
      "Token: extrair, Similaridade: 0\n",
      "Token: informações, Similaridade: 0\n",
      "Token: relevantes, Similaridade: 0\n",
      "Token: grandes, Similaridade: 0\n",
      "Token: volumes, Similaridade: 0\n",
      "Token: utiliza, Similaridade: 0\n",
      "Token: métodos, Similaridade: 0\n",
      "Token: avançados, Similaridade: 0\n",
      "Token: encontrar, Similaridade: 0\n",
      "Token: classificar, Similaridade: 0\n",
      "Token: documentos, Similaridade: 0\n",
      "Token: atendam, Similaridade: 0\n",
      "Token: consultas, Similaridade: 0\n",
      "Token: específicas, Similaridade: 0\n",
      "Token: usuários, Similaridade: 0\n",
      "Token: distância, Similaridade: 0\n",
      "Token: levenshtein, Similaridade: 0\n",
      "Token: modelo, Similaridade: 0\n",
      "Token: duas, Similaridade: 0\n",
      "Token: abordagens, Similaridade: 0\n",
      "Token: distintas, Similaridade: 0\n",
      "Token: eficazes, Similaridade: 0\n",
      "Token: medir, Similaridade: 0\n",
      "Token: similaridade, Similaridade: 0\n",
      "Token: textos, Similaridade: 0\n",
      "Token: calcula, Similaridade: 0\n",
      "Token: número, Similaridade: 0\n",
      "Token: mínimo, Similaridade: 0\n",
      "Token: operações, Similaridade: 0\n",
      "Token: necessárias, Similaridade: 0\n",
      "Token: transformar, Similaridade: 0\n",
      "Token: string, Similaridade: 0\n",
      "Token: outra, Similaridade: 0\n",
      "Token: enquanto, Similaridade: 0\n",
      "Token: representa, Similaridade: 0\n",
      "Token: vetores, Similaridade: 0\n",
      "Token: frequência, Similaridade: 0\n",
      "Token: palavras, Similaridade: 0\n",
      "Token: permitindo, Similaridade: 0\n",
      "Token: comparação, Similaridade: 0\n",
      "Token: baseada, Similaridade: 0\n",
      "Token: vetorial, Similaridade: 0\n",
      "Token: fundamentais, Similaridade: 0\n",
      "Token: melhorar, Similaridade: 0\n",
      "Token: precisão, Similaridade: 0\n",
      "Token: sistemas, Similaridade: 0\n",
      "Token: busca, Similaridade: 0\n",
      "Token: relevância, Similaridade: 0\n",
      "Token: resultados, Similaridade: 0\n",
      "Token: apresentados, Similaridade: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSimilaridades Bag-of-Words:\")\n",
    "for token, similaridade in similaridades.items():\n",
    "    print(f\"Token: {token}, Similaridade: {similaridade}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
